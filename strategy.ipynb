{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import getpass\n",
    "\n",
    "ig_account = input(\"Enter IG account:\")\n",
    "pass_ig_account = getpass.getpass('Enter Password:')\n",
    "ig_api_key = input(\"Enter IG API key:\")\n",
    "\n",
    "#prod server\n",
    "#url = 'https://api.ig.com/gateway/deal'\n",
    "\n",
    "#demo server\n",
    "#url = 'https://demo-api.ig.com/gateway/deal'\n",
    "\n",
    "\n",
    "s = requests.Session()\n",
    "account = {'identifier': ig_account, 'password': pass_ig_account} \n",
    "s.headers = {'Content-Type': 'application/json; charset=UTF-8',\n",
    "            'Accept': 'application/json; charset=UTF-8',\n",
    "            'VERSION': '2',\n",
    "            'X-IG-API-KEY': ig_api_key}\n",
    "\n",
    "#use the 'headers' parameter to set the HTTP headers:\n",
    "r = s.post(url + '/session', json = account)\n",
    "s.headers.update({'X-SECURITY-TOKEN': r.headers['X-SECURITY-TOKEN'],'CST': r.headers['CST']})\n",
    "s.headers.update({'Version': '2'})\n",
    "\n",
    "#prod\n",
    "#url = 'https://api.ig.com/gateway/deal/prices/IX.D.DAX.FWM2.IP/MINUTE_15/2021-07-27 17:00:00/2021-08-27 21:00:00'\n",
    "url = 'https://api.ig.com/gateway/deal/prices/IX.D.DAX.IFS.IP/MINUTE_15/2021-09-13 14:00:00/2021-11-24 22:00:00'\n",
    "\n",
    "\n",
    "\n",
    "#demo\n",
    "\n",
    "#url = 'https://demo-api.ig.com/gateway/deal/prices/IX.D.DAX.FWM2.IP/MINUTE_15/2020-11-09 06:45:00/2020-12-17 12:00:00'\n",
    "#url = 'https://demo-api.ig.com/gateway/deal/prices/IX.D.DAX.FWM2.IP/MINUTE_15/2020-12-15 10:15:00/2020-12-15 10:30:00'\n",
    "#url = 'https://demo-api.ig.com/gateway/deal/prices/IX.D.DAX.FWM2.IP/MINUTE_15/2021-09-13 14:00:00/2021-11-24 22:00:00'\n",
    "\n",
    "#url = 'https://demo-api.ig.com/gateway/deal/prices/IX.D.DAX.FWM2.IP'\n",
    "#use the 'headers' parameter to set the HTTP headers:\n",
    "\n",
    "resp_data = s.get(url)\n",
    "print(resp_data.json())\n",
    "resp_dict = json.loads(resp_data.text)\n",
    "print(type(resp_dict))\n",
    "resp_list = resp_dict[\"prices\"]\n",
    "print(type(resp_list))\n",
    "hist_prices = list(resp_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(pd.json_normalize(hist_prices), orient='columns')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv (r'dax_M15_130921_241121.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dfa = pd.read_csv('dax_M15_010620_051020.csv')\n",
    "dfb = pd.read_csv('dax_M15_051020_171220.csv')\n",
    "dfc = pd.read_csv('dax_M15_171220_290121.csv')\n",
    "dfd = pd.read_csv('dax_M15_290121_120321.csv')\n",
    "dfe = pd.read_csv('dax_M15_140621_270721.csv')\n",
    "dff = pd.read_csv('dax_M15_270721_270821.csv')\n",
    "dfg = pd.read_csv('dax_M15_270821_130921.csv')\n",
    "dfh = pd.read_csv('dax_M15_130921_241121.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate the periods into a common df\n",
    "df = pd.concat([dfa, dfb, dfc, dfd, dfe, dff, dfg, dfh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing only\n",
    "#df_1 = pd.concat([dfe, dff, dfg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df.drop('openPrice.lastTraded', inplace=True, axis=1)\n",
    "df.drop('closePrice.lastTraded', inplace=True, axis=1)\n",
    "df.drop('highPrice.lastTraded', inplace=True, axis=1)\n",
    "df.drop('lowPrice.lastTraded', inplace=True, axis=1)\n",
    "#df.drop('snapshotTime', inplace=True, axis=1)\n",
    "#Calculate mean values\n",
    "df.insert(1, 'openp', ((df['openPrice.ask'] + df['openPrice.bid'])/2).astype(float))\n",
    "df.insert(1, 'closep', ((df['closePrice.ask'] + df['closePrice.bid'])/2).astype(float))\n",
    "df.insert(1, 'highp', ((df['highPrice.ask'] + df['highPrice.bid'])/2).astype(float))\n",
    "df.insert(1, 'lowp', ((df['lowPrice.ask'] + df['lowPrice.bid'])/2).astype(float))\n",
    "df.insert(1, 'midp', ((df['closep'] + df['openp'])/2).astype(float))\n",
    "df.rename(columns={'lastTradedVolume': 'volume'}, inplace=True)\n",
    "df.drop('openPrice.ask', inplace=True, axis=1)\n",
    "df.drop('closePrice.ask', inplace=True, axis=1)\n",
    "df.drop('highPrice.ask', inplace=True, axis=1)\n",
    "df.drop('lowPrice.ask', inplace=True, axis=1)\n",
    "df.drop('openPrice.bid', inplace=True, axis=1)\n",
    "df.drop('closePrice.bid', inplace=True, axis=1)\n",
    "df.drop('highPrice.bid', inplace=True, axis=1)\n",
    "df.drop('lowPrice.bid', inplace=True, axis=1)\n",
    "\n",
    "#Add weighted moving averages\n",
    "#wma10\n",
    "weights = np.arange(1,11) #this creates an array with integers 1 to 10 included\n",
    "wma10 = df['midp'].rolling(10).apply(lambda mid: np.dot(mid, weights)/weights.sum(), raw=True)\n",
    "df.insert(1, 'wma10', (wma10).astype(float))\n",
    "#wma20\n",
    "weights = np.arange(1,21) #this creates an array with integers 1 to 10 included\n",
    "wma20 = df['midp'].rolling(20).apply(lambda mid: np.dot(mid, weights)/weights.sum(), raw=True)\n",
    "df.insert(1, 'wma20', (wma20).astype(float))\n",
    "#wma50\n",
    "weights = np.arange(1,51) #this creates an array with integers 1 to 10 included\n",
    "wma50 = df['midp'].rolling(50).apply(lambda mid: np.dot(mid, weights)/weights.sum(), raw=True)\n",
    "df.insert(1, 'wma50', (wma50).astype(float))\n",
    "#wma100\n",
    "weights = np.arange(1,101) #this creates an array with integers 1 to 10 included\n",
    "wma100 = df['midp'].rolling(100).apply(lambda mid: np.dot(mid, weights)/weights.sum(), raw=True)\n",
    "df.insert(1, 'wma100', (wma100).astype(float))\n",
    "#wma200\n",
    "weights = np.arange(1,201) #this creates an array with integers 1 to 10 included\n",
    "wma200 = df['midp'].rolling(200).apply(lambda mid: np.dot(mid, weights)/weights.sum(), raw=True)\n",
    "df.insert(1, 'wma200', (wma200).astype(float))\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "#df['snapshotTime'] = pd.to_datetime(df['snapshotTime'])\n",
    "\n",
    "# Display\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.options.display.max_rows = 1000\n",
    "df.head(20)\n",
    "#df.to_csv (r'prices_dataframe.csv', index = False, header=True)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop('midp', inplace=True, axis=1)\n",
    "df.drop('midp', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wwma(values, n):\n",
    "    \"\"\"\n",
    "     J. Welles Wilder's EMA \n",
    "    \"\"\"\n",
    "    return values.ewm(alpha=1/n, adjust=False).mean()\n",
    "\n",
    "def atr(df, n=14):\n",
    "    data = df.copy()\n",
    "    high = data['highp']\n",
    "    low = data['lowp']\n",
    "    close = data['closep']\n",
    "    data['tr0'] = abs(high - low)\n",
    "    data['tr1'] = abs(high - close.shift())\n",
    "    data['tr2'] = abs(low - close.shift())\n",
    "    tr = data[['tr0', 'tr1', 'tr2']].max(axis=1)\n",
    "    atr = wwma(tr, n)\n",
    "    return atr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Average True Range (14)\n",
    "atr(df)\n",
    "df['atr14']=atr(df)\n",
    "print(type(df))\n",
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#from mpl_finance import candlestick_ohlc\n",
    "\n",
    "plt.plot(np.array(df['openp'])[10:200], '>')\n",
    "#plt.plot(np.array(df['closep'])[10:200], '<')\n",
    "#plt.plot(np.array(df['closep'])[400:500], 'o')\n",
    "#plt.plot(np.array(df['atr14'])[100:11500], 's')\n",
    "#plt.plot(np.array(df['wma50'])[0:19])\n",
    "#plt.plot(np.array(df['wma200'])[0:19])\n",
    "#plt.plot(np.array(df['day_cos'])[:150])\n",
    "#plt.plot(np.array(df['year_sin'])[:150])\n",
    "#plt.plot(np.array(df['year_cos'])[:150])\n",
    "plt.xlabel('Time [h]')\n",
    "plt.title('Open price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the following lines to only use rows where volume > 200\n",
    "\n",
    "df = df[df.volume > 200]\n",
    "df = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize date time. In this case we will only use the day part (based on wehter the chosen timeseries is e.g. 15min or Daily)\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "#date_time is set to timestamp integer value via the .astype('int64')\n",
    "date_time = pd.to_datetime(df.pop('snapshotTime'),).astype('int64') // 10**9\n",
    "#timestamp_s = date_time.map(datetime.datetime.timestamp)\n",
    "day = 24*60*60\n",
    "week = 7 * day\n",
    "year = (365.2425)*day\n",
    "df['day_sin'] = np.sin(date_time * (2 * np.pi / day))\n",
    "df['week_sin'] = np.sin(date_time * (2 * np.pi / week))\n",
    "#df['day_cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "#df['year_sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "#df['year_cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.array(df['day_sin'])[:401])\n",
    "plt.plot(np.array(df['week_sin'])[:401])\n",
    "#plt.plot(np.array(df['day_cos'])[:150])\n",
    "#plt.plot(np.array(df['year_sin'])[:150])\n",
    "#plt.plot(np.array(df['year_cos'])[:150])\n",
    "plt.xlabel('Time [h]')\n",
    "plt.title('Time of day signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DropExecute if dropping time signals\n",
    "df.drop('day_sin', inplace=True, axis=1)\n",
    "df.drop('week_sin', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build category labels by looping through dataset\n",
    "#Hyperparameters for high and low factor and how manny steps into the future\n",
    "\n",
    "f_look = 6\n",
    "phigh = 1.5\n",
    "plow = 1.5\n",
    "i=0\n",
    "category = []\n",
    "row_count = len(df.index)\n",
    "#print(\"for loop\")\n",
    "while i < row_count:\n",
    "#    print(f'index: {i}')a\n",
    "    hhcompare= df.iloc[i, df.columns.get_loc('closep')] + df.iloc[i, df.columns.get_loc('atr14')]*phigh\n",
    "    llcompare= df.iloc[i, df.columns.get_loc('closep')] - df.iloc[i, df.columns.get_loc('atr14')]*phigh\n",
    "    hlcompare= df.iloc[i, df.columns.get_loc('closep')] + df.iloc[i, df.columns.get_loc('atr14')]*plow\n",
    "    lhcompare= df.iloc[i, df.columns.get_loc('closep')] - df.iloc[i, df.columns.get_loc('atr14')]*plow\n",
    "#    print(hhcompare)\n",
    "#    print(llcompare)\n",
    "#    print(hlcompare)\n",
    "#    print(lhcompare)\n",
    "#    b= (df.loc[i:i+f_look, 'highp'].max() > hhcompare) and (df.loc[i:i+f_look, 'lowp'].min() > lhcompare)\n",
    "#    c= (df.loc[i:i+f_look, 'lowp'].min() < llcompare) and (df.loc[i:i+f_look, 'highp'].max() < hlcompare)\n",
    "    b= (df.loc[i:i+f_look, 'highp'].max() > hhcompare) and (df.loc[i:i+f_look, 'lowp'].min() > lhcompare)\n",
    "    c= (df.loc[i:i+f_look, 'lowp'].min() < llcompare) and (df.loc[i:i+f_look, 'highp'].max() < hlcompare)\n",
    "#    if not(b == c and not b):\n",
    "#        if b :\n",
    "#            b1= df.loc[i:i+10, 'highp'].idxmax()\n",
    "#            print(f'bye is {b1-i}')\n",
    "#            print(df.loc[i:i+10, 'highp'].max())\n",
    "#        elif c :\n",
    "#            c1= df.loc[i:i+10, 'lowp'].idxmin()\n",
    "#            print(f'sell is {c1-i}')\n",
    "#            print(df.loc[i:i+10, 'lowp'].min())\n",
    "#     else :\n",
    "#        print('hold')\n",
    "    if (b == c and not b):\n",
    "        a = True\n",
    "    else:\n",
    "        a = False\n",
    "            \n",
    "    category.append([int(a), int(b), int(c)])\n",
    "    i +=1\n",
    "        \n",
    "pd.reset_option('display.max_rows')\n",
    "pd.options.display.max_rows = 250\n",
    "df_labels = pd.DataFrame(category, columns=[\"hold\", \"bye\", \"sell\"])\n",
    "df_labels.head(230)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling\n",
    "#First calculate the change in pips\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "df['low_pip']=df.lowp-df.lowp.shift(1)\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lowp_z'] = zscore(df['low_pip'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['lowp_rtn'] = minmax_scale(df['lowp_rtn'], feature_range=(-1, 1))\n",
    "#df['highp_minmax'] = minmax_scale(df['highp_rtn'], feature_range=(-1, 1))\n",
    "#df['openp_rtn'] = minmax_scale(df['openp_rtn'], feature_range=(-1, 1))\n",
    "#df['closep_rtn'] = minmax_scale(df['closep_rtn'], feature_range=(-1, 1))\n",
    "#df['highp_z'] = zscore(df['highp_rtn'])\n",
    "#df\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "X = [[ 1., -2.,  2.],\n",
    "     [ -2.,  1.,  3.],\n",
    "     [ 4.,  1., -2.]]\n",
    "transformer = RobustScaler().fit(X)\n",
    "transformer\n",
    "transformer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np_low = df.loc[:,'lowp_rtn']\n",
    "#lowp_rtn_np =  np_low.values\n",
    "mynp = df.loc[:,'low_pip'].values.reshape(-1, 1)\n",
    "#lowp_rtn_np =  np_low.values\n",
    "#print(lowp_rtn_np)\n",
    "#mynp = lowp_rtn_np.reshape(-1, 1)\n",
    "print(mynp)\n",
    "\n",
    "transformer = RobustScaler().fit(mynp)\n",
    "transformer\n",
    "transformer.transform(mynp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp = pd.DataFrame(mynp, columns=[\"low_adj\"])\n",
    "df_mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer\n",
    "RobustScaler()\n",
    "transformer.transform(X)\n",
    "array([[ 0. , -2. ,  0. ],       [-1. ,  0. ,  0.4],       [ 1. ,  0. , -1.6]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vma10_diff'] = df['closep']-df['wma10']\n",
    "df['vma20_diff'] = df['closep']-df['wma20']\n",
    "df['vma50_diff'] = df['closep']-df['wma50']\n",
    "df['vma100_diff'] = df['closep']-df['wma100']\n",
    "df['vma200_diff'] = df['closep']-df['wma200']\n",
    "\n",
    "df['vma10_diff'] = minmax_scale(df['vma10_diff'], feature_range=(-1, 1))\n",
    "df['vma20_diff'] = minmax_scale(df['vma20_diff'], feature_range=(-1, 1))\n",
    "df['vma50_diff'] = minmax_scale(df['vma50_diff'], feature_range=(-1, 1))\n",
    "df['vma100_diff'] = minmax_scale(df['vma100_diff'], feature_range=(-1, 1))\n",
    "df['vma200_diff'] = minmax_scale(df['vma200_diff'], feature_range=(-1, 1))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#print(df['lowp_z'].hist(bins=1000, figsize = (10,5)))\n",
    "print(df_mp.hist(bins=1000, figsize = (10,5)))\n",
    "#print(df['highp_minmax'].hist(bins=1000, figsize = (10,5)))\n",
    "#print(df['highp_rtn'].hist(bins=1000, figsize = (10,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "print(df['highp_z'].hist(bins=1000, figsize = (10,5)))\n",
    "#print(df['highp_minmax'].hist(bins=1000, figsize = (10,5)))\n",
    "#print(df['highp_rtn'].hist(bins=1000, figsize = (10,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(df.highp_minmax > 78).sum()\n",
    "print(df.loc[df['highp_minmax'] > 0.8])\n",
    "#print(df.loc[df['snapshotTime'] == '2021/06/14 08:45:00'])\n",
    "#print(df.loc[[10771]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize ranges with zscore\n",
    "from scipy.stats import zscore\n",
    "df['wma10'] = zscore(df['wma10'])\n",
    "df['wma20'] = zscore(df['wma20'])\n",
    "df['wma50'] = zscore(df['wma50'])\n",
    "df['wma100'] = zscore(df['wma100'])\n",
    "df['wma200'] = zscore(df['wma200'])\n",
    "df['lowp'] = zscore(df['lowp'])\n",
    "df['highp'] = zscore(df['highp'])\n",
    "df['openp'] = zscore(df['openp'])\n",
    "df['closep'] = zscore(df['closep'])\n",
    "df['volume'] = zscore(df['volume'])\n",
    "df['atr14'] = zscore(df['atr14'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the main dataframe and the created labels\n",
    "df = pd.concat([df, df_labels], axis=1)\n",
    "#drop NaN rows\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.hold.count(1)\n",
    "print(df[df['hold']>0].sell.count())\n",
    "print(df[df['sell']>0].bye.count())\n",
    "print(df[df['bye']>0].hold.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove fraction of random \"hold\" rows to balance data\n",
    "mysample = df.query('hold > 0').sample(frac=0.342).index\n",
    "df.drop(mysample , inplace=True)\n",
    "print(df[df['hold']>0].sell.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[['snapshotTime', 'hold', 'bye', 'sell']][17250:17500]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data\n",
    "We'll use a (70%, 20%, 10%) split for the training, validation, and test sets. Note the data is not being randomly shuffled before splitting. This is for two reasons.\n",
    "\n",
    "It ensures that chopping the data into windows of consecutive samples is still possible.\n",
    "It ensures that the validation/test results are more realistic, being evaluated on data collected after the model was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "\n",
    "n = len(df)\n",
    "train_df = df[0:int(n*0.7)]\n",
    "val_df = df[int(n*0.7):int(n*0.9)]\n",
    "test_df = df[int(n*0.9):]\n",
    "\n",
    "num_features = df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recreate the labels frames and drop the colums from the feature df\n",
    "\n",
    "train_labels_df = train_df[['hold','bye', 'sell']]\n",
    "val_labels_df = val_df[['hold','bye', 'sell']]\n",
    "test_labels_df = test_df[['hold','bye', 'sell']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop('hold', inplace=True, axis=1)\n",
    "val_df.drop('hold', inplace=True, axis=1)\n",
    "test_df.drop('hold', inplace=True, axis=1)\n",
    "train_df.drop('bye', inplace=True, axis=1)\n",
    "val_df.drop('bye', inplace=True, axis=1)\n",
    "test_df.drop('bye', inplace=True, axis=1)\n",
    "train_df.drop('sell', inplace=True, axis=1)\n",
    "val_df.drop('sell', inplace=True, axis=1)\n",
    "test_df.drop('sell', inplace=True, axis=1)\n",
    "#df_labels.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.count()\n",
    "#train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequencing with Keras TimeseriesGenerator\n",
    "Create the numpy arrays to be used by the keras models using the Keras TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np\n",
    "\n",
    "lenght = 20\n",
    "\n",
    "#training set\n",
    "data_gen_train = TimeseriesGenerator(train_df.to_numpy(), train_labels_df.to_numpy(),\n",
    "                               length=lenght, sampling_rate=1,\n",
    "                               batch_size=15862)\n",
    "batch_0 = data_gen_train[0]\n",
    "train_np, train_labels_np = batch_0\n",
    "\n",
    "#validation set\n",
    "data_gen_val = TimeseriesGenerator(val_df.to_numpy(), val_labels_df.to_numpy(),\n",
    "                               length=lenght, sampling_rate=1,\n",
    "                               batch_size=15862)\n",
    "batch_0 = data_gen_val[0]\n",
    "val_np, val_labels_np = batch_0\n",
    "\n",
    "#test set\n",
    "data_gen_test = TimeseriesGenerator(test_df.to_numpy(), test_labels_df.to_numpy(),\n",
    "                               length=lenght, sampling_rate=1,\n",
    "                               batch_size=15862)\n",
    "batch_0 = data_gen_test[0]\n",
    "test_np, test_labels_np = batch_0\n",
    "\n",
    "\n",
    "\n",
    "print(train_np.shape)\n",
    "print(train_labels_np.shape)\n",
    "print(val_np.shape)\n",
    "print(val_labels_np.shape)\n",
    "print(test_np.shape)\n",
    "print(test_labels_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, LSTM\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Model configuration\n",
    "batch_size = 6\n",
    "img_width, img_height, img_num_channels = 13, 20, 1\n",
    "loss_function = sparse_categorical_crossentropy\n",
    "no_classes = 3\n",
    "no_epochs = 50\n",
    "optimizer = Adam()\n",
    "validation_split = 0.2\n",
    "verbosity = 1\n",
    "\n",
    "# Determine shape of the data\n",
    "input_shape = (img_width, img_height, img_num_channels)\n",
    "train_np.shape[0]\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    train_np = train_np.reshape(train_np.shape[0], 1, img_height, img_width)\n",
    "    test_np = test_np.reshape(test_np.shape[0], 1, img_height, img_width)\n",
    "    input_shape = (1, img_height, img_width)\n",
    "else:\n",
    "    train_np = train_np.reshape(train_np.shape[0], img_height, img_width, 1)\n",
    "    test_np = test_np.reshape(test_np.shape[0], img_height, img_width, 1)\n",
    "    input_shape = (img_height, img_width, 1)\n",
    " \n",
    "\n",
    "print(train_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_np.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse numbers as floats\n",
    "#input_train = input_train.astype('float32')\n",
    "#input_test = input_test.astype('float32')\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='tanh'))\n",
    "model.add(Dense(no_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(loss=loss_function,\n",
    "#              optimizer=optimizer,\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# Fit data to model\n",
    "#history = model.fit(train_np, train_labels_np,\n",
    "#            batch_size=batch_size,\n",
    "#            epochs=no_epochs,\n",
    "#            verbose=verbosity,\n",
    "#            validation_split=validation_split)\n",
    "\n",
    "# Generate generalization metrics\n",
    "#score = model.evaluate(test_np, test_labels_np, verbose=0)\n",
    "#print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse numbers as floats\n",
    "#input_train = input_train.astype('float32')\n",
    "#input_test = input_test.astype('float32')\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='tanh', input_shape=input_shape))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='tanh'))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='tanh'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='tanh'))\n",
    "#model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(no_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(loss=loss_function,\n",
    "#              optimizer=optimizer,\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# Fit data to model\n",
    "#history = model.fit(train_np, train_labels_np,\n",
    "#            batch_size=batch_size,\n",
    "#            epochs=no_epochs,\n",
    "#            verbose=verbosity,\n",
    "#            validation_split=validation_split)\n",
    "\n",
    "# Generate generalization metrics\n",
    "#score = model.evaluate(test_np, test_labels_np, verbose=0)\n",
    "#print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TimeDistributed, Conv1D\n",
    "model = Sequential()\n",
    "#model.add(TimeDistributed(Conv1D(32, 3, padding='same', strides=2, activation='tanh'), input_shape = input_shape))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='tanh', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='tanh'))\n",
    "#model.add(Conv2D(32, (3, 3), activation='tanh'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "#model.add(LSTM(100))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='tanh'))\n",
    "#model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(no_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(train_np, train_labels_np,\n",
    "          batch_size=batch_size,\n",
    "          epochs=2,\n",
    "          verbose=2,\n",
    "          validation_data=(test_np, test_labels_np))\n",
    "score = model.evaluate(test_np, test_labels_np, verbose=0)\n",
    "print('Test loss: {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "#print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using either GPU or CPU, send the entire dataset.  This might not work on the GPU.\n",
    "# Set the desired TensorFlow output level for this example\n",
    "\n",
    "val_np = val_np.reshape(val_np.shape[0], img_height, img_width, 1)\n",
    "input_shape = (img_height, img_width, 1)\n",
    "score = model.evaluate(val_np, val_labels_np, verbose=0)\n",
    "print('Test loss: {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# For GPU just grab the first 100 images\n",
    "small_x = val_np[810:1905]\n",
    "small_y = val_labels_np[810:1905]\n",
    "small_y2 = np.argmax(small_y,axis=1)\n",
    "pred = model.predict(small_x)\n",
    "\n",
    "#pred = np.argmax(pred,axis=1)\n",
    "#score = metrics.accuracy_score(small_y2, pred)\n",
    "\n",
    "print('Accuracy: {}'.format(score))\n",
    "#print(pred)\n",
    "#print(f\"Shape: {pred.shape}\")\n",
    "print(pred[500:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(small_y[800:1300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(small_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
